from datasets import load_dataset

squad_v2_ds = load_dataset("rajpurkar/squad_v2")


import torch
from transformers import BertTokenizer, BertForTokenClassification
from transformers import pipeline

def create_pseudo_labels(sentences, model, tokenizer):
    """ 生成伪标注数据，这里简单将所有识别的实体标记为同一类别 """
    pseudo_labels = []
    for sentence in sentences:
        inputs = tokenizer(sentence, return_tensors="pt")
        with torch.no_grad():
            outputs = model(**inputs)
        logits = outputs.logits
        label_ids = logits.argmax(-1).squeeze().tolist()  # 选择最高概率的标签
        pseudo_labels.append(label_ids)
    return pseudo_labels

# 加载预训练的BERT模型和分词器
model_name = "bert-base-uncased"
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertForTokenClassification.from_pretrained(model_name, num_labels=2)  # 假设有两个标签：非实体和实体

# 未标注的数据集
sentences = [
    "Hello, my name is Alice and I live in Paris.",
    "This is an example of using BERT for NER."
]

# 创建伪标注数据
pseudo_labels = create_pseudo_labels(sentences, model, tokenizer)

# 对模型进行微调（这里简化处理，实际需要使用适当的训练循环）
# 注意：在实际使用中，应该添加模型训练代码，这里仅为展示结构

# 使用pipeline进行NER预测
ner_pipeline = pipeline("ner", model=model, tokenizer=tokenizer)

# 预测
for sentence in sentences:
    print(sentence)
    results = ner_pipeline(sentence)
    print(results)
